{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:teal\"> Deep Reinforcement Learning for Atari Enduro-v0 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, concatenate, Permute\n",
    "from keras.layers import Input, Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.activations import relu, linear\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### RoadRunner Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Enduro-v0')\n",
    "\n",
    "env.render()\n",
    "sleep(1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. *Number of possible action*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Possible actoin is : 9\n"
     ]
    }
   ],
   "source": [
    "nb_actions = env.action_space.n\n",
    "print('Total number of Possible actoin is :', nb_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. *Taking stack of 4 consecutive frames*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape is : (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "frame_shape = (84, 84)\n",
    "window_length = 4\n",
    "input_shape = (window_length,) + frame_shape\n",
    "print('Input Shape is :', input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "### Defining class for pre-processing the game_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameProcess(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        img = Image.fromarray(observation)\n",
    "        img = np.array(img.resize(frame_shape).convert('L'))\n",
    "        return img.astype('uint8')  \n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        Processed_batch = batch.astype('float32') / 255.\n",
    "        return Processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## DeepMind Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jshch\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 4617      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 1,688,745\n",
      "Trainable params: 1,688,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "model.add(Conv2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), strides=(1, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Configuring the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. *Allocating memory for experience replay*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000000, window_length=window_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.* Policy: Epsilon Greedy Exploration*\n",
    "<span style=\"color:teal\">*Gradually exploration will be decreased*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. *Compiling DQN Agent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory, processor=GameProcess(),\n",
    "               nb_steps_warmup=50000, gamma=.99, target_model_update=10000, train_interval=4, delta_clip=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(lr=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <span style=\"color:teal\"> Training the model </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. *Check if Agent is learning for first 0.5M Steps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 42s 4ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - ale.lives: 0.000\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 27s 3ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - ale.lives: 0.000\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 28s 3ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - ale.lives: 0.000\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 27s 3ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - ale.lives: 0.000\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 27s 3ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - ale.lives: 0.000\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "    1/10000 [..............................] - ETA: 2:09 - reward: 0.0000e+00WARNING:tensorflow:From C:\\Users\\jshch\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "10000/10000 [==============================] - 104s 10ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mean_absolute_error: 0.015 - mean_q: 0.007 - mean_eps: 0.951 - ale.lives: 0.000\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 100s 10ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.006 - mean_eps: 0.942 - ale.lives: 0.000\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 99s 10ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.006 - mean_eps: 0.933 - ale.lives: 0.000\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 98s 10ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.006 - mean_eps: 0.924 - ale.lives: 0.000\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 99s 10ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.006 - mean_eps: 0.915 - ale.lives: 0.000\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 100s 10ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.006 - mean_eps: 0.906 - ale.lives: 0.000\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 100s 10ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.006 - mean_eps: 0.897 - ale.lives: 0.000\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 100s 10ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.005 - mean_eps: 0.888 - ale.lives: 0.000\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 101s 10ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.006 - mean_eps: 0.879 - ale.lives: 0.000\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 100s 10ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.005 - mean_eps: 0.870 - ale.lives: 0.000\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 101s 10ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.005 - mean_eps: 0.861 - ale.lives: 0.000\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 101s 10ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.005 - mean_eps: 0.852 - ale.lives: 0.000\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 107s 11ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.006 - mean_eps: 0.843 - ale.lives: 0.000\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 108s 11ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.005 - mean_eps: 0.834 - ale.lives: 0.000\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 101s 10ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.006 - mean_eps: 0.825 - ale.lives: 0.000\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 104s 10ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.005 - mean_eps: 0.816 - ale.lives: 0.000\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 105s 10ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.005 - mean_eps: 0.807 - ale.lives: 0.000\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 100s 10ms/step - reward: 0.0000e+000s - rewa\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.005 - mean_eps: 0.798 - ale.lives: 0.000\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 103s 10ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.005 - mean_eps: 0.789 - ale.lives: 0.000\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 104s 10ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.005 - mean_eps: 0.780 - ale.lives: 0.000\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 104s 10ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.005 - mean_eps: 0.771 - ale.lives: 0.000\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 102s 10ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.005 - mean_eps: 0.762 - ale.lives: 0.000\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 104s 10ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.004 - mean_q: -0.005 - mean_eps: 0.753 - ale.lives: 0.000\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 108s 11ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.004 - mean_q: -0.005 - mean_eps: 0.744 - ale.lives: 0.000\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 104s 10ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.004 - mean_q: -0.004 - mean_eps: 0.735 - ale.lives: 0.000\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 98s 10ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.004 - mean_q: -0.004 - mean_eps: 0.726 - ale.lives: 0.000\n",
      "\n",
      "Interval 32 (310000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 98s 10ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.004 - mean_q: -0.004 - mean_eps: 0.717 - ale.lives: 0.000\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      " 6973/10000 [===================>..........] - ETA: 30s - reward: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "history = dqn.fit(env, nb_steps=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. *Summarizing the training history*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['episode_reward'])\n",
    "plt.title('Training for 0.3 million steps')\n",
    "plt.legend(['Episode reward'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['nb_episode_steps'])\n",
    "plt.title('Training for 0.3 million steps')\n",
    "plt.legend(['No. of episode steps'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. *Saving the weights*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_atari_Enduro.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">**-  -  Caution   -  -**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:teal\">Re-Training the model (for 2M steps)  </span>\n",
    "*Loading the saved weights (of 0.3M steps)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('dqn_atari_Enduro.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory, processor=GameProcess(),\n",
    "               gamma=.99, target_model_update=10000, train_interval=4, delta_clip=1.)\n",
    "\n",
    "dqn.compile(Adam(lr=0.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2M = dqn.fit(env, nb_steps=2000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Summarizing the training history*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2M.history['episode_reward'])\n",
    "plt.title('Training for 2 million steps')\n",
    "plt.legend(['Episode reward'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history2M.history['nb_episode_steps'])\n",
    "plt.title('Training for 2 million steps')\n",
    "plt.legend(['No. of episode steps'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. *Saving final weights*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_atari_Enduro.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <span style=\"color:teal\"> Testing the model </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Loading the weights for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('dqn_atari_Enduro.h5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Resetting the environment for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "dqn.test(env, nb_episodes=2, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained on: Intel® Xeon® Processor E5, 2.40 GHz, Nvidia Quadro K4200\n",
    "# Bhartendu Thakur, Machine Learning & Computing\n",
    "# https://in.mathworks.com/matlabcentral/profile/authors/10083740-bhartendu?&detail=fileexchange\n",
    "# https://in.linkedin.com/in/bhartendu-thakur-56bb6285"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
